{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google's PageRank algorithm\n",
    "\n",
    "Pagerank, Google's original killer app, is based on eigen-decomposition. Let's see how it works.\n",
    "\n",
    "*Note: Original notebook from Jonas Kersulis, University of Michigan: https://github.com/kersulis/551-pagerank*\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we begin\n",
    "\n",
    "Run the following cell to install and load the packages we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:30.374021",
     "start_time": "2016-09-22T00:45:28.871533"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from surfer import *\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code in `surfer`, you can generate a graph by crawling the internet. See original notebook for example. *For the purpose of this activity, we will load a stored version of the web-site link graph instead, which has been crawled by starting at `http://eecs.umich.edu` and crawling 500 pages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:33.110310",
     "start_time": "2016-09-22T00:45:32.728325"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reading from file\n",
    "G = nx.read_gml(\"data/eecs.gml\")\n",
    "\n",
    "f = open('data/links.txt')\n",
    "links = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:33.610693",
     "start_time": "2016-09-22T00:45:33.596677"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = nx.adjacency_matrix(G, links).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `spy(A)` to see the connectivity pattern of the adjacency matrix of the web-ste graph.\n",
    "\n",
    "Note that `A` is an n-by-n matrix with `A[i,j] = 1` if site `i` is linked to node `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:35.854091",
     "start_time": "2016-09-22T00:45:34.431135"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = figure(figsize=(6,6))\n",
    "spy(A, markersize=1, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple intuitive ways to rank the web pages.\n",
    "\n",
    "1. **Rank by number of incoming links (in degree):** `sum(A,0)` produces a row vector by summing together the rows of the matrix (computing a row sum); each entry of this vector is the number of other sites that links to the corresponding site).\n",
    "2. **Rank by number of outgoing links (out degree):** `sum(A,1)` produces a column vector; each entry of this vector is the number of sites linked to by the corresponding node.\n",
    "\n",
    "When we sum $A$ by rows or columns, we don't automatically see the links. The graph package we're using makes it easy to see the links along with in or out degree. Run the following cell to see the ten sites with the most incoming links. You can do the same for out degree by replacing `G.in_degree_iter` with `G.out_degree_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:39.298937",
     "start_time": "2016-09-22T00:45:39.290632"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display links sorted by in degree:\n",
    "din_sort = sorted(G.in_degree_iter(), key=lambda i: i[1], reverse=True)\n",
    "din_sort[:10] # top 10 sites by in degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which site is linked to by the most other sites? Which site links to the most other sites?**\n",
    "\n",
    "Do the numbers make sense given the total number of sites (`len(G.nodes()`) and links between them (`len(G.edges()`)?\n",
    "\n",
    "Which of these rankings seems better? How many sites have no outgoing links, and how should they be ranked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to rank pages\n",
    "The ranking approaches just discussed are easily exploited.\n",
    "\n",
    "* With in degree ranking, someone might set up a thousand pages that link to one page, thereby inflating its rank.\n",
    "* With out degree ranking, someone might include a mountain of links on every page they make to inflate its rank.\n",
    "\n",
    "We know there is a deeper meaning in our internet graph. Some pages are more important than others. Degree ranking misses a key component of website importance: the number of sites linking to a particular site matters less than the _importance_ of those sites.\n",
    "\n",
    "Our third approach, pagerank, captures this underlying meaning by focusing on probability rather than degree. If one visits site A, what is the probability they will visit site B next? We can infer these probabilities from the adjacency matrix of the graph. Given these probabilities, we can find the page rank of each node by an iterative procedure,\n",
    "$$\n",
    "PR^n(p_i)=\\alpha\\sum_{p_j\\in In(p_i)} \\left ( PR^{n-1}(p_j)\\times \\frac{1}{|Out(p_j)|} \\right ) + (1-\\alpha)tp,\n",
    "$$\n",
    "where $n$ is the number of nodes in the graph, $PR^n(p_i)$ and $PR^{n-1}(p_i)$ are the current and last iteration pagerank scores for node/page $p_i$, $In(p_i)$ and $Out(p_i)$ are the in- and out-degrees of the node, respectively, $(1-\\alpha)$ is a dampening factor with $0 < \\alpha < 1$, and $tp$ is the teleportation probability (i.e., the probability of jumping to a random node instead of following an outgoing link, usually $1/n$). Generally, $\\alpha$ is close to 1, e.g., 0.8 or 0.9. Note that $\\frac{1}{|Out(p_j)|}$ is in fact the probability of transition from $p_j$ to $p_i$.\n",
    "\n",
    "If we put all pagerank scores in a vector, we can write the above formula as,\n",
    "$$\n",
    "\\mathbf{r}^n = \\alpha \\mathbf{H} \\mathbf{r}^{n-1} + (1-\\alpha)\\mathbf{1}/n,\n",
    "$$\n",
    "where $\\mathbf{r}^n$ and $\\mathbf{r}^{n-1}$ are the pagerank vectors from the current and previous iterations, $\\mathbf{H}$ is the transition probablity matrix derived from the graph adjacency matrix, and $\\mathbf{1}$ is a vector of all 1's of the appropriate size. We set the initial transition probabilities for all nodes to $1/n$, i.e., $\\mathbf{r}^0 = \\mathbf{1}/n$.\n",
    "\n",
    "Given enough iterations, the series converges to the leading eigenvector of the matrix\n",
    "$$\n",
    "\\alpha H + (1-\\alpha)\\mathbf{1}/n,\n",
    "$$\n",
    "which the original [paper](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) from Google solved via the `power iteration` method, which we will see in a bit. First, we must deal with edge-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dangling nodes\n",
    "Nodes that have an incoming link but no outgoing link are referred to as *dangling nodes* and can cause problems in computing the series (why?). We can \"fix\" this by forcing every page to be connected to itself, allowing each page to have at least one in-link and out-link. The adjacency matrix produced by `surfer()` will only have a 1 in the `[i,i]` entry if a page explicitly links to itself, so our web adjacency matrix may have dangling nodes. The function belowsets the diagonal of the adjacency matrix to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:41.810192",
     "start_time": "2016-09-22T00:45:41.807104"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_dangling_nodes(A):\n",
    "    \"\"\"\n",
    "    Return input matrix A (square) with all diagonal elements set to 1.\n",
    "    \"\"\"\n",
    "    return tril(A, -1) + triu(A, 1) + eye(size(A, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the probability matrix\n",
    "Our goal is to have a matrix where each element $H[i,j]$ is the probability of transitioning from node $i$ to node $j$. Let $G$ be the adjacency matrix with dangling nodes fixed. Then $H$ is generated from $G$ by normalizing the rows to have sum 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:42.389858",
     "start_time": "2016-09-22T00:45:42.379898"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_rows(G):\n",
    "    \"\"\"\n",
    "    Return input matrix A normalized so each row\n",
    "    represents probability of leaving node\n",
    "    (each row of returned matrix sums to one)\n",
    "    \"\"\"\n",
    "    out_degree = sum(G, 1)\n",
    "    H = copy(G)\n",
    "    for i in range(len(out_degree)):\n",
    "        if out_degree[i] > 0:\n",
    "            H[i,:] = G[i,:]/out_degree[i]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating 0 elements\n",
    "\n",
    "We're almost ready to apply power iteration, but first we need to address the zeros in our probability matrix. The pagerank vector is defined as the leading eigenvector of the matrix\n",
    "\n",
    "$$\n",
    "\\alpha H + (1-\\alpha)\\mathbf{1}/n.\n",
    "$$\n",
    "\n",
    "To ensure this matrix can be properly de-composed, we replace all 0 probabilities with a very small value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:43.044695",
     "start_time": "2016-09-22T00:45:43.037229"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_zeros(H, alpha):\n",
    "    return alpha*H + (1 - alpha)*ones(shape(H))/size(H,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together: adjacency to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:43.922698",
     "start_time": "2016-09-22T00:45:43.910326"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H = normalize_rows(fix_dangling_nodes(A))\n",
    "alpha = 0.85\n",
    "Ha = remove_zeros(H, alpha).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we took the transpose (by adding `.T`), so now $H[i,j]$ is the probability of transitioning from node $j$ to node $i$. You should be comfortable with manipulations like this -- we're only doing it so we can work with right Eigenvectors rather than left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power iteration\n",
    "We're going to do what Google first did: use power iteration to compute the leading eigenvector of $H_\\alpha$, and use this vector to rank our web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:47.154050",
     "start_time": "2016-09-22T00:45:47.142721"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def power_iteration(Ha, steps):\n",
    "    \"\"\"\n",
    "    Beginning with random starting vector, perform specified\n",
    "    number of power iteration steps. Return final vector.\n",
    "    \"\"\"\n",
    "    u = rand(size(Ha, 1))\n",
    "    u = u/norm(u)\n",
    "\n",
    "    for idx in range(steps):\n",
    "        u = dot(Ha, u)\n",
    "        u = u/norm(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:47.964950",
     "start_time": "2016-09-22T00:45:47.944172"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = power_iteration(Ha, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing rankings\n",
    "Run the following cell to create and show a table containing all out in degree, out degree, and pagerank data. Click on a column heading to sort by that column (double-click to sort in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:45:55.822625",
     "start_time": "2016-09-22T00:45:55.633321"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "din_vec = [G.in_degree()[l] for l in links]\n",
    "dout_vec = [G.out_degree()[l] for l in links]\n",
    "\n",
    "df = pd.DataFrame({ 'in_degree' : din_vec,\n",
    "                    'out_degree' : dout_vec,\n",
    "                    'pagerank' : r }, index=links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:50:09.131617",
     "start_time": "2016-09-22T00:50:09.115936"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display top 10 sites by each sorting method\n",
    "\n",
    "df.sort_values(by='in_degree', ascending=False).head(10)\n",
    "# df.sort_values(by='out_degree', ascending=False).head(10)\n",
    "# df.sort_values(by='pagerank', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What URL has the second highest pagerank? Does it make sense or is that a bug of `surfer()`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why pagerank wins\n",
    "It may not be clear that pagerank is better than degree sorting. After all, sites like umich.edu and eecs.umich.edu rise to the top in all three rankings. To see why pagerank is superior, consider the site http://eecs.umich.edu/cse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:47:49.499841",
     "start_time": "2016-09-22T00:47:49.494427"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc['http://eecs.umich.edu/cse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our graph, this site has an in degree of 1, an out degree of 69, and a pagerank value of 0.002386. It is near the bottom of the list on all three rankings.\n",
    "\n",
    "Now suppose http://umich.edu and http://regents.umich.edu linked to http://eecs.umich.edu/cse. This would indicate that http://eecs.umich.edu/cse is a very important site. Would our rankings capture that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:49:08.990226",
     "start_time": "2016-09-22T00:49:08.781746"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add our hypothetical graph edges:\n",
    "G.add_edge('http://umich.edu','http://eecs.umich.edu/cse')\n",
    "G.add_edge('http://regents.umich.edu','http://eecs.umich.edu/cse')\n",
    "\n",
    "# recompute adjacency matrix:\n",
    "A2 = nx.adjacency_matrix(G, links).todense()\n",
    "\n",
    "# compute transition matrix Ha:\n",
    "H2 = normalize_rows(fix_dangling_nodes(A2))\n",
    "alpha = 0.85\n",
    "Ha2 = remove_zeros(H2, alpha).T\n",
    "\n",
    "# perform power iteration:\n",
    "r2 = power_iteration(Ha2, 200)\n",
    "\n",
    "# update data table:\n",
    "din_vec = [G.in_degree()[l] for l in links]\n",
    "dout_vec = [G.out_degree()[l] for l in links]\n",
    "\n",
    "df2 = pd.DataFrame({ 'in_degree' : din_vec,\n",
    "                    'out_degree' : dout_vec,\n",
    "                    'pagerank' : r2 }, index=links)\n",
    "\n",
    "df2.sort_values(by='pagerank', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T00:48:32.277536",
     "start_time": "2016-09-22T00:48:32.270896"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2.loc['http://eecs.umich.edu/cse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happened?\n",
    "\n",
    "* In degree increased from 1 to 3, a modest increase.\n",
    "* Out degree remained the same.\n",
    "* Pagerank value increased from 0.002386 to 0.19197, a significant boost!\n",
    "\n",
    "Despite being linked to by two of the most important pages, http://eecs.umich.edu/cse did not reach the top of in degree or out degree rankings. But its pagerank boost was huge: its new value of 0.191969 makes it the 5th highest site by pagerank!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
